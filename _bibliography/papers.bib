---
---
@inproceedings{kim2024sfec,
  abbr = {AAAI},
  bibtex_show = {true},
  title={Salient Frequency-aware Exemplar Compression for Resource-constrained Online Continual Learning}, 
  author={Kim, Junsu and Kim, Suhyun},
  booktitle={The 39th Annual AAAI Conference on Artificial Intelligence (AAAI)},
  abstract={Online Class-Incremental Learning (OCIL) enables a model to learn new classes from a data stream. Since data stream samples are seen only once and the capacity of storage is constrained, OCIL is particularly susceptible to Catastrophic Forgetting (CF). While exemplar replay methods alleviate CF by storing representative samples, the limited capacity of the buffer inhibits capturing the entire old data distribution, leading to CF. In this regard, recent papers suggest image compression for better memory usage. However, existing methods raise two concerns: computational overhead and compression defects. On one hand, computational overhead can limit their applicability in OCIL settings, as models might miss learning opportunities from the current streaming data if computational resources are budgeted and preoccupied with compression. On the other hand, typical compression schemes demanding low computational overhead, such as JPEG, introduce noise detrimental to training. To address these issues, we propose Salient Frequency-aware Exemplar Compression (SFEC), an efficient and effective JPEG-based compression framework. SFEC exploits saliency information in the frequency domain to reduce negative impacts from compression artifacts for learning. Moreover, SFEC employs weighted sampling for exemplar elimination based on the distance between raw and compressed data to mitigate artifacts further. Our experiments employing the baseline OCIL method on benchmark datasets such as CIFAR-100 and Mini-ImageNet demonstrate the superiority of SFEC over previous exemplar compression methods in streaming scenarios.
},
  kind = {conference},
  selected = {true},
  year={2025},
  pdf = {kim2024SFEC.pdf}
}



@inproceedings{gil2024tlp,
  abbr = {ESL},
  bibtex_show = {true},
  title={TLP Balancer: Predictive Thread Allocation for Multi-Tenant Inference in Embedded GPUs}, 
  author={Gil, Minseong and Jeon, Jaebeom and  and Kim, Junsu and Choi, Sangun and Koo, Gunjae and Yoon, Myung Kuk and Oh, Yunho},
  booktitle={The IEEE Embedded Systems Letters},
  abstract={This paper introduces a novel software technique to optimize thread allocation for merged and fused kernels in multi-tenant inference systems on embedded Graphics Processing Units (GPUs).
Embedded systems equipped with GPUs face challenges in managing diverse deep learning workloads while adhering to Quality-of-Service (QoS) standards, primarily due to limited hardware resources and the varied nature of deep learning models. 
Prior work has relied on static thread allocation strategies, often leading to suboptimal hardware utilization. 
To address these challenges, we propose a new software technique called TLP Balancer. 
TLP Balancer automatically identifies the best-performing number of threads based on performance modeling. 
This approach significantly enhances hardware utilization and ensures QoS compliance, outperforming traditional fixed-thread allocation methods. 
Our evaluation shows that TLP Balancer improves throughput by 40\% compared to the state-of-the-art automated kernel merge and fusion techniques.
},
  kind = {conference},
  selected = {true},
  year={2024},
  pdf = {gil2024tlp.pdf},
}


@inproceedings{jeon2024vitbit,
  abbr = {ICPP},
  bibtex_show = {true},
  title={VitBit: Enhancing Embedded GPU Performance for AI Workloads through Register Operand Packing}, 
  author={Jeon, Jaebeom and Gil, Minseong and Kim, Junsu and Park, Jaeyong and Koo, Gunjae and Yoon, Myung Kuk and Oh, Yunho},
  booktitle={The 53rd International Conference on Parallel Processing (ICPP)},
  abstract={The rapid advancement of Artificial Intelligence (AI) necessitates
significant enhancements in the energy efficiency of Graphics Pro-
cessing Units (GPUs) for Deep Neural Network (DNN) workloads.
Such a challenge is particularly critical for embedded GPUs, which
operate within stringent power constraints. Traditional GPU ar-
chitectures, designed to support a limited set of numeric formats,
face challenges in meeting the diverse requirements of modern
AI applications. These applications demand support for various
numeric formats to optimize computational speed and efficiency.
This paper proposes VitBit, a novel software technique designed
to overcome these limitations by enabling efficient processing of
arbitrary integer format values, especially those 8 bits or fewer,
which are increasingly prevalent in AI workloads. VitBit introduces
two key innovations: the packing of arbitrary integer formats for
parallel computation and the simultaneous execution of Tensor
cores, INT and FP (Integer and Floating-Point) CUDA cores. This
approach leverages the architectural features of modern GPUs, such
as those based on NVIDIA Ampere architecture, which allows con-
current operation of FP32 and INT32 cores at full throughput. Our
evaluation of VitBit on NVIDIA Jetson AGX Orin demonstrates
substantial improvements in arithmetic density and peak through-
put, achieving up to a 22% reduction in execution time for bench-
mark AI workloads without compromising computational accuracy.
VitBit effectively bridges the gap between current hardware capa-
bilities and the computational demands of AI, offering a scalable
and cost-effective method for enhancing GPU performance in AI
applications.},
  kind = {conference},
  selected = {true},
  year={2024},
  pdf = {jeon2024vitbit.pdf},
}



@inproceedings{kim2021hammerfilter,
  abbr = {ICCD},
  bibtex_show = {true},
  title={HammerFilter: Robust Protection and Low Hardware Overhead Method for RowHammer}, 
  author={Kim, Kwangrae and Woo, Jeonghyun and Kim, Junsu and Chung, Ki-Seok},
  booktitle={2021 IEEE 39th International Conference on Computer Design (ICCD)},
  pages={212-219},
  abstract={The continuous scaling-down of the dynamic random access memory (DRAM) manufacturing process has made it possible to improve DRAM density. However, it makes small DRAM cells susceptible to electromagnetic interference between nearby cells. Unless DRAM cells are adequately isolated from each other, the frequent switching access of some cells may lead to unintended bit flips in adjacent cells. This phenomenon is commonly referred to as RowHammer. It is often considered a security issue because unusually frequent accesses to a small set of rows generated by malicious attacks can cause bit flips. Such bit flips may also be caused by general applications. Although several solutions have been proposed, most approaches either incur excessive area overhead or exhibit limited prevention capabilities against maliciously crafted attack patterns. Therefore, the goals of this study are (1) to mitigate RowHammer, even when the number of aggressor rows increases and attack patterns become complicated, and (2) to implement the method with a low area overhead.We propose a robust hardware-based protection method for RowHammer attacks with a low hardware cost called HammerFilter, which employs a modified version of the counting bloom filter. It tracks all attacking rows efficiently by leveraging the fact that the counting bloom filter is a space-efficient data structure, and we add an operation, HALF-DELETE, to mitigate the energy overhead. According to our experimental results, the proposed method can completely prevent bit flips when facing artificially crafted attack patterns (five patterns in our experiments), whereas state-of-the-art probabilistic solutions can only mitigate less than 56% of bit flips on average. Furthermore, the proposed method has a much lower area cost compared to existing counter-based solutions (40.6&#x00D7; better than TWiCe and 2.3&#x00D7; better than Graphene).},
  doi={10.1109/ICCD53106.2021.00043},
  ISSN={2576-6996},
  kind = {conference},
  selected = {true},
  year={2021},
  pdf = {kim2021hammerfilter.pdf},
  html = {https://ieeexplore.ieee.org/abstract/document/9643804},
  slides = {21iccd_HammerFilter_slides.pptx},
  video = {https://www.youtube.com/watch?v=X_O8YaOSKLE}
}


@inproceedings{kim2024most,
  abbr = {P1},
  title={Memory Oversubscription-aware Tensor Migration Scheduling for GPU Unified Storage Architecture}, 
  author={author, 1st},
  booktitle={IEEE Computer Architecture Letters (Under Review in CAL)},
  kind = {journal},
  selected = {false},
}

@inproceedings{kim2024CXL,
  abbr = {P2},
  title={Mitigating Software Overhead in Tiered Memory-based Accelerator for Training}, 
  author={co-author},
  booktitle={52th The International Symposium on Computer Architecture (Under Review in ISCA'25)},
  kind = {conference},
  selected = {false},
}

@inproceedings{kim2024CXL,
  abbr = {P3},
  title={Hardware Supports for Processing Arbitrary Numeric Formats on GPUs}, 
  author={co-author},
  booktitle={52th The International Symposium on Computer Architecture (Under Review in ISCA'25)},
  kind = {conference},
  selected = {false},
}

@inproceedings{kim2024CXL,
  abbr = {P4},
  title={A Behavioral Analysis of CXL Memory Systems}, 
  author={co-author},
  booktitle={ACM SIGMETRICS 2025 (Under Review in SIGMETRICS'25)},
  kind = {conference},
  selected = {false},
}

@inproceedings{kim2024yinyang,
  abbr = {P5},
  title={Accelerating Yinyang K-Means on Heterogeneous Platform}, 
  author={co-author},
  booktitle={39th IEEE International Parallel & Distributed Processing Symposium (Under Review in IPDPS'25, got 3 borderlines)},
  kind = {conference},
  selected = {false},
}

@article{kim2024dfcil,
  abbr = {P6},
  title={Improving Performance of Data-Free Continual Learning}, 
  author={co-author},
  booktitle={Ready for Submission},
  kind = {conference},
  selected = {false},
  year={2024},
}



